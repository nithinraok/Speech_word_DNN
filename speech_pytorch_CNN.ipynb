{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import  transforms,datasets\n",
    "from torch.utils import data\n",
    "import librosa\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bed': 0, 'bird': 1, 'cat': 2, 'dog': 3, 'down': 4, 'eight': 5, 'five': 6, 'four': 7, 'go': 8, 'happy': 9, 'house': 10, 'left': 11, 'marvin': 12, 'nine': 13, 'no': 14, 'off': 15, 'on': 16, 'one': 17, 'right': 18, 'seven': 19, 'sheila': 20, 'six': 21, 'stop': 22, 'three': 23, 'tree': 24, 'two': 25, 'up': 26, 'wow': 27, 'yes': 28, 'zero': 29}\n"
     ]
    }
   ],
   "source": [
    "categories = !ls Data/speech_commands/train | sort -u \n",
    "categories=categories[1:]\n",
    "category_dict={cat:i for i,cat in enumerate(categories)}\n",
    "print(category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition={}\n",
    "df=pd.read_csv('Data/speech_commands/csvs/train.csv')\n",
    "partition['train']=df['file_name'].get_values()\n",
    "partition['train_labels']=df['label'].get_values()\n",
    "\n",
    "df=pd.read_csv('Data/speech_commands/csvs/valid.csv')\n",
    "partition['validation']=df['file_name'].get_values()\n",
    "partition['validation_labels']=df['label'].get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self,filenames,labels):\n",
    "        self.filenames=filenames\n",
    "        self.labels=labels\n",
    "        self.sr=16000\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        file = self.filenames[index]\n",
    "        wav,_ = librosa.load(file,self.sr)\n",
    "        wav = librosa.util.normalize(wav)\n",
    "        wav = librosa.util.pad_center(wav,self.sr)\n",
    "        mfcc_feat= librosa.feature.mfcc(wav,self.sr,n_mfcc=13)\n",
    "        delta_feat = mfcc_feat[:-1]-mfcc_feat[1:]\n",
    "#         mfcc_delta = librosa.feature.delta(mfcc,order=2)\n",
    "        deltadelta_feat = delta_feat[:-1]-delta_feat[1:]\n",
    "        #Removing the first two frames\n",
    "        mfcc_feat = mfcc_feat[2:]\n",
    "        delta_feat = delta_feat[1:]\n",
    "\n",
    "        full_input = np.concatenate((mfcc_feat,delta_feat,deltadelta_feat), axis=0)\n",
    "\n",
    "        mfcc = torch.from_numpy(full_input).float().unsqueeze(0)\n",
    "        \n",
    "        y = self.labels[index]\n",
    "        return mfcc,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=AudioDataset(partition['train'],partition['train_labels'])\n",
    "valid_dataset=AudioDataset(partition['validation'],partition['validation_labels'])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "valid_loader = data.DataLoader(valid_dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 33, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "x,y=next(iter(train_loader))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,3,(3,3),stride=(1,1),padding=1)\n",
    "        self.conv2 = nn.Conv2d(3,5,(3,3),stride=(1,1),padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d((2,2))\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1=nn.Linear(768,512)\n",
    "        self.fc2=nn.Linear(512,256)\n",
    "        self.fc3=nn.Linear(256,30)\n",
    "        self.out= nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.pool(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "#         x=self.conv2(x)\n",
    "#         x=self.pool(x)\n",
    "#         x=self.dropout(x)\n",
    "#         x=self.relu(x)\n",
    "        \n",
    "        x=x.view(x.shape[0],-1) # This selects the batch size x 1\n",
    "        \n",
    "        x=self.fc1(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        x=self.fc2(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        x=self.fc3(x)\n",
    "        x=self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=30, bias=True)\n",
      "  (out): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Model,trainloader,testloader,criterion,optimizer,epochs):\n",
    "    test_losses,train_losses=[],[]\n",
    "    for e in range(epochs):\n",
    "        running_loss=0;\n",
    "\n",
    "        Model.train();\n",
    "        for images,labels in trainloader:\n",
    "#             images_t = images.view(images.shape[0],-1);\n",
    "            optimizer.zero_grad();\n",
    "\n",
    "            logits=Model.forward(images);\n",
    "            loss_t=criterion(logits,labels);\n",
    "            loss_t.backward();\n",
    "            optimizer.step();\n",
    "\n",
    "            running_loss+=loss_t;\n",
    "\n",
    "        else:\n",
    "            test_loss,accuracy=validation(Model,testloader,criterion);\n",
    "\n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "                      \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                      \"Test Accuracy: {:.3f}\".format(accuracy))\n",
    "\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "\n",
    "    return train_losses,test_losses,accuracy\n",
    "\n",
    "def validation(Model,testloader,criterion):\n",
    "    test_loss=0;\n",
    "    accuracy=0;\n",
    "\n",
    "    Model.eval();\n",
    "    images_num=0;\n",
    "    with torch.no_grad():\n",
    "        for images,labels in testloader:\n",
    "            images_num+=images.shape[0];\n",
    "#             images_t=images.view(images.shape[0],-1);\n",
    "            logits=Model.forward(images);\n",
    "            loss_t=criterion(logits,labels)\n",
    "            test_loss+=loss_t;\n",
    "\n",
    "            _,pred_labels=torch.topk(logits,1,dim=1)\n",
    "            equality=(labels==pred_labels.view(*labels.shape))\n",
    "\n",
    "            accuracy += torch.sum(equality)\n",
    "\n",
    "    overall_acc=accuracy.float()/images_num;\n",
    "    return test_loss,overall_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer=torch.optim.Adam(Model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5..  Training Loss: 3.033..  Test Loss: 2.175..  Test Accuracy: 0.363\n",
      "Epoch: 2/5..  Training Loss: 2.269..  Test Loss: 1.651..  Test Accuracy: 0.532\n",
      "Epoch: 3/5..  Training Loss: 1.943..  Test Loss: 1.375..  Test Accuracy: 0.607\n",
      "Epoch: 4/5..  Training Loss: 1.750..  Test Loss: 1.189..  Test Accuracy: 0.664\n",
      "Epoch: 5/5..  Training Loss: 1.613..  Test Loss: 1.074..  Test Accuracy: 0.690\n"
     ]
    }
   ],
   "source": [
    "train_loss,test_loss,accuracy=train(Model,train_loader,valid_loader,\n",
    "                                             criterion,optimizer,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
